{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "paragraph = \"\"\"The authors \\n are indebted to the following people for feedback on earlier drafts of this\n",
    "book: Doug Arnold, Michaela Atterer, Greg Aumann, Kenneth Beesley, Steven Bethard,\n",
    "Ondrej Bojar, Chris Cieri, Robin Cooper, Grev Corbett, James Curran, Dan Garrette,\n",
    "Jean Mark Gawron, Doug Hellmann, Nitin Indurkhya, Mark Liberman, Peter Ljunglöf,\n",
    "Stefan Müller, Robin Munn, Joel Nothman, Adam Przepiorkowski, Brandon Rhodes,\n",
    "Stuart Robinson, Jussi Salmela, Kyle Schlansker, Rob Speer, and Richard Sproat. We\n",
    "are thankful to many students and colleagues for their comments on the class materials\n",
    "that evolved into these chapters, including participants at NLP and linguistics summer\n",
    "schools in Brazil, India, and the USA. This book would not exist without the members\n",
    "of the nltk-dev developer community, named on the NLTK website, who have given\n",
    "so freely of their time and expertise in building and extending NLTK.\n",
    "We are grateful to the U.S. National Science Foundation, the Linguistic Data Consortium, an Edward Clarence Dyason Fellowship, and the Universities of Pennsylvania,\n",
    "Edinburgh, and Melbourne for supporting our work on this book.\n",
    "We thank Julie Steele, Abby Fox, Loranah Dimant, and the rest of the O’Reilly team,\n",
    "for organizing comprehensive reviews of our drafts from people across the NLP and\n",
    "Python communities, for cheerfully customizing O’Reilly’s production tools to accommodate our needs, and for meticulous copyediting work.\n",
    "Finally, we owe a huge debt of gratitude to our partners, Kay, Mimo, and Jee, for their\n",
    "love, patience, and support over the many years that we worked on this book. We hope that pages our children—Andrew, Alison, Kirsten, Leonie, and Maaike—catch our enthusiasm for language and computation from these pages.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'authors',\n",
       " 'are',\n",
       " 'indebted',\n",
       " 'to',\n",
       " 'the',\n",
       " 'following',\n",
       " 'people',\n",
       " 'for',\n",
       " 'feedback',\n",
       " 'on',\n",
       " 'earlier',\n",
       " 'drafts',\n",
       " 'of',\n",
       " 'this',\n",
       " 'book',\n",
       " ':',\n",
       " 'Doug',\n",
       " 'Arnold',\n",
       " ',',\n",
       " 'Michaela',\n",
       " 'Atterer',\n",
       " ',',\n",
       " 'Greg',\n",
       " 'Aumann',\n",
       " ',',\n",
       " 'Kenneth',\n",
       " 'Beesley',\n",
       " ',',\n",
       " 'Steven',\n",
       " 'Bethard',\n",
       " ',',\n",
       " 'Ondrej',\n",
       " 'Bojar',\n",
       " ',',\n",
       " 'Chris',\n",
       " 'Cieri',\n",
       " ',',\n",
       " 'Robin',\n",
       " 'Cooper',\n",
       " ',',\n",
       " 'Grev',\n",
       " 'Corbett',\n",
       " ',',\n",
       " 'James',\n",
       " 'Curran',\n",
       " ',',\n",
       " 'Dan',\n",
       " 'Garrette',\n",
       " ',',\n",
       " 'Jean',\n",
       " 'Mark',\n",
       " 'Gawron',\n",
       " ',',\n",
       " 'Doug',\n",
       " 'Hellmann',\n",
       " ',',\n",
       " 'Nitin',\n",
       " 'Indurkhya',\n",
       " ',',\n",
       " 'Mark',\n",
       " 'Liberman',\n",
       " ',',\n",
       " 'Peter',\n",
       " 'Ljunglöf',\n",
       " ',',\n",
       " 'Stefan',\n",
       " 'Müller',\n",
       " ',',\n",
       " 'Robin',\n",
       " 'Munn',\n",
       " ',',\n",
       " 'Joel',\n",
       " 'Nothman',\n",
       " ',',\n",
       " 'Adam',\n",
       " 'Przepiorkowski',\n",
       " ',',\n",
       " 'Brandon',\n",
       " 'Rhodes',\n",
       " ',',\n",
       " 'Stuart',\n",
       " 'Robinson',\n",
       " ',',\n",
       " 'Jussi',\n",
       " 'Salmela',\n",
       " ',',\n",
       " 'Kyle',\n",
       " 'Schlansker',\n",
       " ',',\n",
       " 'Rob',\n",
       " 'Speer',\n",
       " ',',\n",
       " 'and',\n",
       " 'Richard',\n",
       " 'Sproat',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'thankful',\n",
       " 'to',\n",
       " 'many',\n",
       " 'students',\n",
       " 'and',\n",
       " 'colleagues',\n",
       " 'for',\n",
       " 'their',\n",
       " 'comments',\n",
       " 'on',\n",
       " 'the',\n",
       " 'class',\n",
       " 'materials',\n",
       " 'that',\n",
       " 'evolved',\n",
       " 'into',\n",
       " 'these',\n",
       " 'chapters',\n",
       " ',',\n",
       " 'including',\n",
       " 'participants',\n",
       " 'at',\n",
       " 'NLP',\n",
       " 'and',\n",
       " 'linguistics',\n",
       " 'summer',\n",
       " 'schools',\n",
       " 'in',\n",
       " 'Brazil',\n",
       " ',',\n",
       " 'India',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'USA',\n",
       " '.',\n",
       " 'This',\n",
       " 'book',\n",
       " 'would',\n",
       " 'not',\n",
       " 'exist',\n",
       " 'without',\n",
       " 'the',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'nltk-dev',\n",
       " 'developer',\n",
       " 'community',\n",
       " ',',\n",
       " 'named',\n",
       " 'on',\n",
       " 'the',\n",
       " 'NLTK',\n",
       " 'website',\n",
       " ',',\n",
       " 'who',\n",
       " 'have',\n",
       " 'given',\n",
       " 'so',\n",
       " 'freely',\n",
       " 'of',\n",
       " 'their',\n",
       " 'time',\n",
       " 'and',\n",
       " 'expertise',\n",
       " 'in',\n",
       " 'building',\n",
       " 'and',\n",
       " 'extending',\n",
       " 'NLTK',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'grateful',\n",
       " 'to',\n",
       " 'the',\n",
       " 'U.S.',\n",
       " 'National',\n",
       " 'Science',\n",
       " 'Foundation',\n",
       " ',',\n",
       " 'the',\n",
       " 'Linguistic',\n",
       " 'Data',\n",
       " 'Consortium',\n",
       " ',',\n",
       " 'an',\n",
       " 'Edward',\n",
       " 'Clarence',\n",
       " 'Dyason',\n",
       " 'Fellowship',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Universities',\n",
       " 'of',\n",
       " 'Pennsylvania',\n",
       " ',',\n",
       " 'Edinburgh',\n",
       " ',',\n",
       " 'and',\n",
       " 'Melbourne',\n",
       " 'for',\n",
       " 'supporting',\n",
       " 'our',\n",
       " 'work',\n",
       " 'on',\n",
       " 'this',\n",
       " 'book',\n",
       " '.',\n",
       " 'We',\n",
       " 'thank',\n",
       " 'Julie',\n",
       " 'Steele',\n",
       " ',',\n",
       " 'Abby',\n",
       " 'Fox',\n",
       " ',',\n",
       " 'Loranah',\n",
       " 'Dimant',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'O',\n",
       " '’',\n",
       " 'Reilly',\n",
       " 'team',\n",
       " ',',\n",
       " 'for',\n",
       " 'organizing',\n",
       " 'comprehensive',\n",
       " 'reviews',\n",
       " 'of',\n",
       " 'our',\n",
       " 'drafts',\n",
       " 'from',\n",
       " 'people',\n",
       " 'across',\n",
       " 'the',\n",
       " 'NLP',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'communities',\n",
       " ',',\n",
       " 'for',\n",
       " 'cheerfully',\n",
       " 'customizing',\n",
       " 'O',\n",
       " '’',\n",
       " 'Reilly',\n",
       " '’',\n",
       " 's',\n",
       " 'production',\n",
       " 'tools',\n",
       " 'to',\n",
       " 'accommodate',\n",
       " 'our',\n",
       " 'needs',\n",
       " ',',\n",
       " 'and',\n",
       " 'for',\n",
       " 'meticulous',\n",
       " 'copyediting',\n",
       " 'work',\n",
       " '.',\n",
       " 'Finally',\n",
       " ',',\n",
       " 'we',\n",
       " 'owe',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'debt',\n",
       " 'of',\n",
       " 'gratitude',\n",
       " 'to',\n",
       " 'our',\n",
       " 'partners',\n",
       " ',',\n",
       " 'Kay',\n",
       " ',',\n",
       " 'Mimo',\n",
       " ',',\n",
       " 'and',\n",
       " 'Jee',\n",
       " ',',\n",
       " 'for',\n",
       " 'their',\n",
       " 'love',\n",
       " ',',\n",
       " 'patience',\n",
       " ',',\n",
       " 'and',\n",
       " 'support',\n",
       " 'over',\n",
       " 'the',\n",
       " 'many',\n",
       " 'years',\n",
       " 'that',\n",
       " 'we',\n",
       " 'worked',\n",
       " 'on',\n",
       " 'this',\n",
       " 'book',\n",
       " '.',\n",
       " 'We',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'pages',\n",
       " 'our',\n",
       " 'children—Andrew',\n",
       " ',',\n",
       " 'Alison',\n",
       " ',',\n",
       " 'Kirsten',\n",
       " ',',\n",
       " 'Leonie',\n",
       " ',',\n",
       " 'and',\n",
       " 'Maaike—catch',\n",
       " 'our',\n",
       " 'enthusiasm',\n",
       " 'for',\n",
       " 'language',\n",
       " 'and',\n",
       " 'computation',\n",
       " 'from',\n",
       " 'these',\n",
       " 'pages',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(paragraph)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('authors', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('indebted', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('following', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('feedback', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('earlier', 'JJR'),\n",
       " ('drafts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('book', 'NN'),\n",
       " (':', ':'),\n",
       " ('Doug', 'NNP'),\n",
       " ('Arnold', 'NNP'),\n",
       " (',', ','),\n",
       " ('Michaela', 'NNP'),\n",
       " ('Atterer', 'NNP'),\n",
       " (',', ','),\n",
       " ('Greg', 'NNP'),\n",
       " ('Aumann', 'NNP'),\n",
       " (',', ','),\n",
       " ('Kenneth', 'NNP'),\n",
       " ('Beesley', 'NNP'),\n",
       " (',', ','),\n",
       " ('Steven', 'NNP'),\n",
       " ('Bethard', 'NNP'),\n",
       " (',', ','),\n",
       " ('Ondrej', 'NNP'),\n",
       " ('Bojar', 'NNP'),\n",
       " (',', ','),\n",
       " ('Chris', 'NNP'),\n",
       " ('Cieri', 'NNP'),\n",
       " (',', ','),\n",
       " ('Robin', 'NNP'),\n",
       " ('Cooper', 'NNP'),\n",
       " (',', ','),\n",
       " ('Grev', 'NNP'),\n",
       " ('Corbett', 'NNP'),\n",
       " (',', ','),\n",
       " ('James', 'NNP'),\n",
       " ('Curran', 'NNP'),\n",
       " (',', ','),\n",
       " ('Dan', 'NNP'),\n",
       " ('Garrette', 'NNP'),\n",
       " (',', ','),\n",
       " ('Jean', 'NNP'),\n",
       " ('Mark', 'NNP'),\n",
       " ('Gawron', 'NNP'),\n",
       " (',', ','),\n",
       " ('Doug', 'NNP'),\n",
       " ('Hellmann', 'NNP'),\n",
       " (',', ','),\n",
       " ('Nitin', 'NNP'),\n",
       " ('Indurkhya', 'NNP'),\n",
       " (',', ','),\n",
       " ('Mark', 'NNP'),\n",
       " ('Liberman', 'NNP'),\n",
       " (',', ','),\n",
       " ('Peter', 'NNP'),\n",
       " ('Ljunglöf', 'NNP'),\n",
       " (',', ','),\n",
       " ('Stefan', 'NNP'),\n",
       " ('Müller', 'NNP'),\n",
       " (',', ','),\n",
       " ('Robin', 'NNP'),\n",
       " ('Munn', 'NNP'),\n",
       " (',', ','),\n",
       " ('Joel', 'NNP'),\n",
       " ('Nothman', 'NNP'),\n",
       " (',', ','),\n",
       " ('Adam', 'NNP'),\n",
       " ('Przepiorkowski', 'NNP'),\n",
       " (',', ','),\n",
       " ('Brandon', 'NNP'),\n",
       " ('Rhodes', 'NNP'),\n",
       " (',', ','),\n",
       " ('Stuart', 'NNP'),\n",
       " ('Robinson', 'NNP'),\n",
       " (',', ','),\n",
       " ('Jussi', 'NNP'),\n",
       " ('Salmela', 'NNP'),\n",
       " (',', ','),\n",
       " ('Kyle', 'NNP'),\n",
       " ('Schlansker', 'NNP'),\n",
       " (',', ','),\n",
       " ('Rob', 'NNP'),\n",
       " ('Speer', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('Richard', 'NNP'),\n",
       " ('Sproat', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('thankful', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('many', 'JJ'),\n",
       " ('students', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('colleagues', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('comments', 'NNS'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('class', 'NN'),\n",
       " ('materials', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('evolved', 'VBD'),\n",
       " ('into', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('chapters', 'NNS'),\n",
       " (',', ','),\n",
       " ('including', 'VBG'),\n",
       " ('participants', 'NNS'),\n",
       " ('at', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('linguistics', 'NNS'),\n",
       " ('summer', 'NN'),\n",
       " ('schools', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('Brazil', 'NNP'),\n",
       " (',', ','),\n",
       " ('India', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('USA', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('exist', 'VB'),\n",
       " ('without', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('members', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('nltk-dev', 'JJ'),\n",
       " ('developer', 'NN'),\n",
       " ('community', 'NN'),\n",
       " (',', ','),\n",
       " ('named', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('website', 'NN'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('have', 'VBP'),\n",
       " ('given', 'VBN'),\n",
       " ('so', 'RB'),\n",
       " ('freely', 'RB'),\n",
       " ('of', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('time', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('expertise', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('building', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('extending', 'VBG'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('grateful', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('U.S.', 'NNP'),\n",
       " ('National', 'NNP'),\n",
       " ('Science', 'NNP'),\n",
       " ('Foundation', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('Linguistic', 'NNP'),\n",
       " ('Data', 'NNP'),\n",
       " ('Consortium', 'NNP'),\n",
       " (',', ','),\n",
       " ('an', 'DT'),\n",
       " ('Edward', 'NNP'),\n",
       " ('Clarence', 'NNP'),\n",
       " ('Dyason', 'NNP'),\n",
       " ('Fellowship', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('Universities', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('Pennsylvania', 'NNP'),\n",
       " (',', ','),\n",
       " ('Edinburgh', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('Melbourne', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('supporting', 'VBG'),\n",
       " ('our', 'PRP$'),\n",
       " ('work', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('thank', 'VBD'),\n",
       " ('Julie', 'NNP'),\n",
       " ('Steele', 'NNP'),\n",
       " (',', ','),\n",
       " ('Abby', 'NNP'),\n",
       " ('Fox', 'NNP'),\n",
       " (',', ','),\n",
       " ('Loranah', 'NNP'),\n",
       " ('Dimant', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('rest', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('O', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('Reilly', 'NNP'),\n",
       " ('team', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('organizing', 'VBG'),\n",
       " ('comprehensive', 'JJ'),\n",
       " ('reviews', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('drafts', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('people', 'NNS'),\n",
       " ('across', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('NLP', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Python', 'NNP'),\n",
       " ('communities', 'NNS'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('cheerfully', 'RB'),\n",
       " ('customizing', 'VBG'),\n",
       " ('O', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('Reilly', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'JJ'),\n",
       " ('production', 'NN'),\n",
       " ('tools', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('accommodate', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('needs', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('for', 'IN'),\n",
       " ('meticulous', 'JJ'),\n",
       " ('copyediting', 'VBG'),\n",
       " ('work', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Finally', 'RB'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('owe', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('huge', 'JJ'),\n",
       " ('debt', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('gratitude', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('our', 'PRP$'),\n",
       " ('partners', 'NNS'),\n",
       " (',', ','),\n",
       " ('Kay', 'NNP'),\n",
       " (',', ','),\n",
       " ('Mimo', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('Jee', 'NNP'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('love', 'NN'),\n",
       " (',', ','),\n",
       " ('patience', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('support', 'NN'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('many', 'JJ'),\n",
       " ('years', 'NNS'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('worked', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('hope', 'VBP'),\n",
       " ('that', 'IN'),\n",
       " ('pages', 'NNS'),\n",
       " ('our', 'PRP$'),\n",
       " ('children—Andrew', 'NN'),\n",
       " (',', ','),\n",
       " ('Alison', 'NNP'),\n",
       " (',', ','),\n",
       " ('Kirsten', 'NNP'),\n",
       " (',', ','),\n",
       " ('Leonie', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('Maaike—catch', 'NNP'),\n",
       " ('our', 'PRP$'),\n",
       " ('enthusiasm', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('language', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('computation', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('pages', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partsofspeech\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The_DT',\n",
       " 'authors_NNS',\n",
       " 'are_VBP',\n",
       " 'indebted_VBN',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'following_JJ',\n",
       " 'people_NNS',\n",
       " 'for_IN',\n",
       " 'feedback_NN',\n",
       " 'on_IN',\n",
       " 'earlier_JJR',\n",
       " 'drafts_NNS',\n",
       " 'of_IN',\n",
       " 'this_DT',\n",
       " 'book_NN',\n",
       " ':_:',\n",
       " 'Doug_NNP',\n",
       " 'Arnold_NNP',\n",
       " ',_,',\n",
       " 'Michaela_NNP',\n",
       " 'Atterer_NNP',\n",
       " ',_,',\n",
       " 'Greg_NNP',\n",
       " 'Aumann_NNP',\n",
       " ',_,',\n",
       " 'Kenneth_NNP',\n",
       " 'Beesley_NNP',\n",
       " ',_,',\n",
       " 'Steven_NNP',\n",
       " 'Bethard_NNP',\n",
       " ',_,',\n",
       " 'Ondrej_NNP',\n",
       " 'Bojar_NNP',\n",
       " ',_,',\n",
       " 'Chris_NNP',\n",
       " 'Cieri_NNP',\n",
       " ',_,',\n",
       " 'Robin_NNP',\n",
       " 'Cooper_NNP',\n",
       " ',_,',\n",
       " 'Grev_NNP',\n",
       " 'Corbett_NNP',\n",
       " ',_,',\n",
       " 'James_NNP',\n",
       " 'Curran_NNP',\n",
       " ',_,',\n",
       " 'Dan_NNP',\n",
       " 'Garrette_NNP',\n",
       " ',_,',\n",
       " 'Jean_NNP',\n",
       " 'Mark_NNP',\n",
       " 'Gawron_NNP',\n",
       " ',_,',\n",
       " 'Doug_NNP',\n",
       " 'Hellmann_NNP',\n",
       " ',_,',\n",
       " 'Nitin_NNP',\n",
       " 'Indurkhya_NNP',\n",
       " ',_,',\n",
       " 'Mark_NNP',\n",
       " 'Liberman_NNP',\n",
       " ',_,',\n",
       " 'Peter_NNP',\n",
       " 'Ljunglöf_NNP',\n",
       " ',_,',\n",
       " 'Stefan_NNP',\n",
       " 'Müller_NNP',\n",
       " ',_,',\n",
       " 'Robin_NNP',\n",
       " 'Munn_NNP',\n",
       " ',_,',\n",
       " 'Joel_NNP',\n",
       " 'Nothman_NNP',\n",
       " ',_,',\n",
       " 'Adam_NNP',\n",
       " 'Przepiorkowski_NNP',\n",
       " ',_,',\n",
       " 'Brandon_NNP',\n",
       " 'Rhodes_NNP',\n",
       " ',_,',\n",
       " 'Stuart_NNP',\n",
       " 'Robinson_NNP',\n",
       " ',_,',\n",
       " 'Jussi_NNP',\n",
       " 'Salmela_NNP',\n",
       " ',_,',\n",
       " 'Kyle_NNP',\n",
       " 'Schlansker_NNP',\n",
       " ',_,',\n",
       " 'Rob_NNP',\n",
       " 'Speer_NNP',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'Richard_NNP',\n",
       " 'Sproat_NNP',\n",
       " '._.',\n",
       " 'We_PRP',\n",
       " 'are_VBP',\n",
       " 'thankful_JJ',\n",
       " 'to_TO',\n",
       " 'many_JJ',\n",
       " 'students_NNS',\n",
       " 'and_CC',\n",
       " 'colleagues_NNS',\n",
       " 'for_IN',\n",
       " 'their_PRP$',\n",
       " 'comments_NNS',\n",
       " 'on_IN',\n",
       " 'the_DT',\n",
       " 'class_NN',\n",
       " 'materials_NNS',\n",
       " 'that_WDT',\n",
       " 'evolved_VBD',\n",
       " 'into_IN',\n",
       " 'these_DT',\n",
       " 'chapters_NNS',\n",
       " ',_,',\n",
       " 'including_VBG',\n",
       " 'participants_NNS',\n",
       " 'at_IN',\n",
       " 'NLP_NNP',\n",
       " 'and_CC',\n",
       " 'linguistics_NNS',\n",
       " 'summer_NN',\n",
       " 'schools_NNS',\n",
       " 'in_IN',\n",
       " 'Brazil_NNP',\n",
       " ',_,',\n",
       " 'India_NNP',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'the_DT',\n",
       " 'USA_NNP',\n",
       " '._.',\n",
       " 'This_DT',\n",
       " 'book_NN',\n",
       " 'would_MD',\n",
       " 'not_RB',\n",
       " 'exist_VB',\n",
       " 'without_IN',\n",
       " 'the_DT',\n",
       " 'members_NNS',\n",
       " 'of_IN',\n",
       " 'the_DT',\n",
       " 'nltk-dev_JJ',\n",
       " 'developer_NN',\n",
       " 'community_NN',\n",
       " ',_,',\n",
       " 'named_VBN',\n",
       " 'on_IN',\n",
       " 'the_DT',\n",
       " 'NLTK_NNP',\n",
       " 'website_NN',\n",
       " ',_,',\n",
       " 'who_WP',\n",
       " 'have_VBP',\n",
       " 'given_VBN',\n",
       " 'so_RB',\n",
       " 'freely_RB',\n",
       " 'of_IN',\n",
       " 'their_PRP$',\n",
       " 'time_NN',\n",
       " 'and_CC',\n",
       " 'expertise_NN',\n",
       " 'in_IN',\n",
       " 'building_NN',\n",
       " 'and_CC',\n",
       " 'extending_VBG',\n",
       " 'NLTK_NNP',\n",
       " '._.',\n",
       " 'We_PRP',\n",
       " 'are_VBP',\n",
       " 'grateful_JJ',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'U.S._NNP',\n",
       " 'National_NNP',\n",
       " 'Science_NNP',\n",
       " 'Foundation_NNP',\n",
       " ',_,',\n",
       " 'the_DT',\n",
       " 'Linguistic_NNP',\n",
       " 'Data_NNP',\n",
       " 'Consortium_NNP',\n",
       " ',_,',\n",
       " 'an_DT',\n",
       " 'Edward_NNP',\n",
       " 'Clarence_NNP',\n",
       " 'Dyason_NNP',\n",
       " 'Fellowship_NNP',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'the_DT',\n",
       " 'Universities_NNS',\n",
       " 'of_IN',\n",
       " 'Pennsylvania_NNP',\n",
       " ',_,',\n",
       " 'Edinburgh_NNP',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'Melbourne_NNP',\n",
       " 'for_IN',\n",
       " 'supporting_VBG',\n",
       " 'our_PRP$',\n",
       " 'work_NN',\n",
       " 'on_IN',\n",
       " 'this_DT',\n",
       " 'book_NN',\n",
       " '._.',\n",
       " 'We_PRP',\n",
       " 'thank_VBD',\n",
       " 'Julie_NNP',\n",
       " 'Steele_NNP',\n",
       " ',_,',\n",
       " 'Abby_NNP',\n",
       " 'Fox_NNP',\n",
       " ',_,',\n",
       " 'Loranah_NNP',\n",
       " 'Dimant_NNP',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'the_DT',\n",
       " 'rest_NN',\n",
       " 'of_IN',\n",
       " 'the_DT',\n",
       " 'O_NNP',\n",
       " '’_NNP',\n",
       " 'Reilly_NNP',\n",
       " 'team_NN',\n",
       " ',_,',\n",
       " 'for_IN',\n",
       " 'organizing_VBG',\n",
       " 'comprehensive_JJ',\n",
       " 'reviews_NNS',\n",
       " 'of_IN',\n",
       " 'our_PRP$',\n",
       " 'drafts_NNS',\n",
       " 'from_IN',\n",
       " 'people_NNS',\n",
       " 'across_IN',\n",
       " 'the_DT',\n",
       " 'NLP_NNP',\n",
       " 'and_CC',\n",
       " 'Python_NNP',\n",
       " 'communities_NNS',\n",
       " ',_,',\n",
       " 'for_IN',\n",
       " 'cheerfully_RB',\n",
       " 'customizing_VBG',\n",
       " 'O_NNP',\n",
       " '’_NNP',\n",
       " 'Reilly_NNP',\n",
       " '’_NNP',\n",
       " 's_JJ',\n",
       " 'production_NN',\n",
       " 'tools_NNS',\n",
       " 'to_TO',\n",
       " 'accommodate_VB',\n",
       " 'our_PRP$',\n",
       " 'needs_NNS',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'for_IN',\n",
       " 'meticulous_JJ',\n",
       " 'copyediting_VBG',\n",
       " 'work_NN',\n",
       " '._.',\n",
       " 'Finally_RB',\n",
       " ',_,',\n",
       " 'we_PRP',\n",
       " 'owe_VBP',\n",
       " 'a_DT',\n",
       " 'huge_JJ',\n",
       " 'debt_NN',\n",
       " 'of_IN',\n",
       " 'gratitude_NN',\n",
       " 'to_TO',\n",
       " 'our_PRP$',\n",
       " 'partners_NNS',\n",
       " ',_,',\n",
       " 'Kay_NNP',\n",
       " ',_,',\n",
       " 'Mimo_NNP',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'Jee_NNP',\n",
       " ',_,',\n",
       " 'for_IN',\n",
       " 'their_PRP$',\n",
       " 'love_NN',\n",
       " ',_,',\n",
       " 'patience_NN',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'support_NN',\n",
       " 'over_IN',\n",
       " 'the_DT',\n",
       " 'many_JJ',\n",
       " 'years_NNS',\n",
       " 'that_IN',\n",
       " 'we_PRP',\n",
       " 'worked_VBD',\n",
       " 'on_IN',\n",
       " 'this_DT',\n",
       " 'book_NN',\n",
       " '._.',\n",
       " 'We_PRP',\n",
       " 'hope_VBP',\n",
       " 'that_IN',\n",
       " 'pages_NNS',\n",
       " 'our_PRP$',\n",
       " 'children—Andrew_NN',\n",
       " ',_,',\n",
       " 'Alison_NNP',\n",
       " ',_,',\n",
       " 'Kirsten_NNP',\n",
       " ',_,',\n",
       " 'Leonie_NNP',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'Maaike—catch_NNP',\n",
       " 'our_PRP$',\n",
       " 'enthusiasm_NN',\n",
       " 'for_IN',\n",
       " 'language_NN',\n",
       " 'and_CC',\n",
       " 'computation_NN',\n",
       " 'from_IN',\n",
       " 'these_DT',\n",
       " 'pages_NNS',\n",
       " '._.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tags = []\n",
    "for tw in tagged_words:\n",
    "    words_tags.append(tw[0]+\"_\"+tw[1])\n",
    "words_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The_DT authors_NNS are_VBP indebted_VBN to_TO the_DT following_JJ people_NNS for_IN feedback_NN on_IN earlier_JJR drafts_NNS of_IN this_DT book_NN :_: Doug_NNP Arnold_NNP ,_, Michaela_NNP Atterer_NNP ,_, Greg_NNP Aumann_NNP ,_, Kenneth_NNP Beesley_NNP ,_, Steven_NNP Bethard_NNP ,_, Ondrej_NNP Bojar_NNP ,_, Chris_NNP Cieri_NNP ,_, Robin_NNP Cooper_NNP ,_, Grev_NNP Corbett_NNP ,_, James_NNP Curran_NNP ,_, Dan_NNP Garrette_NNP ,_, Jean_NNP Mark_NNP Gawron_NNP ,_, Doug_NNP Hellmann_NNP ,_, Nitin_NNP Indurkhya_NNP ,_, Mark_NNP Liberman_NNP ,_, Peter_NNP Ljunglöf_NNP ,_, Stefan_NNP Müller_NNP ,_, Robin_NNP Munn_NNP ,_, Joel_NNP Nothman_NNP ,_, Adam_NNP Przepiorkowski_NNP ,_, Brandon_NNP Rhodes_NNP ,_, Stuart_NNP Robinson_NNP ,_, Jussi_NNP Salmela_NNP ,_, Kyle_NNP Schlansker_NNP ,_, Rob_NNP Speer_NNP ,_, and_CC Richard_NNP Sproat_NNP ._. We_PRP are_VBP thankful_JJ to_TO many_JJ students_NNS and_CC colleagues_NNS for_IN their_PRP$ comments_NNS on_IN the_DT class_NN materials_NNS that_WDT evolved_VBD into_IN these_DT chapters_NNS ,_, including_VBG participants_NNS at_IN NLP_NNP and_CC linguistics_NNS summer_NN schools_NNS in_IN Brazil_NNP ,_, India_NNP ,_, and_CC the_DT USA_NNP ._. This_DT book_NN would_MD not_RB exist_VB without_IN the_DT members_NNS of_IN the_DT nltk-dev_JJ developer_NN community_NN ,_, named_VBN on_IN the_DT NLTK_NNP website_NN ,_, who_WP have_VBP given_VBN so_RB freely_RB of_IN their_PRP$ time_NN and_CC expertise_NN in_IN building_NN and_CC extending_VBG NLTK_NNP ._. We_PRP are_VBP grateful_JJ to_TO the_DT U.S._NNP National_NNP Science_NNP Foundation_NNP ,_, the_DT Linguistic_NNP Data_NNP Consortium_NNP ,_, an_DT Edward_NNP Clarence_NNP Dyason_NNP Fellowship_NNP ,_, and_CC the_DT Universities_NNS of_IN Pennsylvania_NNP ,_, Edinburgh_NNP ,_, and_CC Melbourne_NNP for_IN supporting_VBG our_PRP$ work_NN on_IN this_DT book_NN ._. We_PRP thank_VBD Julie_NNP Steele_NNP ,_, Abby_NNP Fox_NNP ,_, Loranah_NNP Dimant_NNP ,_, and_CC the_DT rest_NN of_IN the_DT O_NNP ’_NNP Reilly_NNP team_NN ,_, for_IN organizing_VBG comprehensive_JJ reviews_NNS of_IN our_PRP$ drafts_NNS from_IN people_NNS across_IN the_DT NLP_NNP and_CC Python_NNP communities_NNS ,_, for_IN cheerfully_RB customizing_VBG O_NNP ’_NNP Reilly_NNP ’_NNP s_JJ production_NN tools_NNS to_TO accommodate_VB our_PRP$ needs_NNS ,_, and_CC for_IN meticulous_JJ copyediting_VBG work_NN ._. Finally_RB ,_, we_PRP owe_VBP a_DT huge_JJ debt_NN of_IN gratitude_NN to_TO our_PRP$ partners_NNS ,_, Kay_NNP ,_, Mimo_NNP ,_, and_CC Jee_NNP ,_, for_IN their_PRP$ love_NN ,_, patience_NN ,_, and_CC support_NN over_IN the_DT many_JJ years_NNS that_IN we_PRP worked_VBD on_IN this_DT book_NN ._. We_PRP hope_VBP that_IN pages_NNS our_PRP$ children—Andrew_NN ,_, Alison_NNP ,_, Kirsten_NNP ,_, Leonie_NNP ,_, and_CC Maaike—catch_NNP our_PRP$ enthusiasm_NN for_IN language_NN and_CC computation_NN from_IN these_DT pages_NNS ._.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_paragraph = ' '.join(words_tags)\n",
    "tagged_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nameentityrecognition\n",
    "\n",
    "#words\n",
    "#tagged_words\n",
    "nameent = nltk.ne_chunk(tagged_words)\n",
    "nameent.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pargraph = \"\"\"Organization Georgia Pacific Cargo., who\n",
    "PERSON Eddy Bonte, President Obama\n",
    "LOCATION Butwal Ktm\n",
    "Date June July\n",
    "Time Two fifty am,pm\n",
    "money 175 million canadaina dollar\n",
    "percent twenty pct, 18.5%\n",
    "facility washingtion momument\n",
    "gpe south asia, midlation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(pargraph)\n",
    "tag_word = nltk.pos_tag(words)\n",
    "tag_word\n",
    "nameent = nltk.ne_chunk(tag_word)\n",
    "nameent.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words\n",
    "#paragraph\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the authors are indebted to the following people for feedback on earlier drafts of this book doug arnold michaela atterer greg aumann kenneth beesley steven bethard ondrej bojar chris cieri robin cooper grev corbett james curran dan garrette jean mark gawron doug hellmann nitin indurkhya mark liberman peter ljunglöf stefan müller robin munn joel nothman adam przepiorkowski brandon rhodes stuart robinson jussi salmela kyle schlansker rob speer and richard sproat ',\n",
       " 'we are thankful to many students and colleagues for their comments on the class materials that evolved into these chapters including participants at nlp and linguistics summer schools in brazil india and the usa ',\n",
       " 'this book would not exist without the members of the nltk dev developer community named on the nltk website who have given so freely of their time and expertise in building and extending nltk ',\n",
       " 'we are grateful to the u s national science foundation the linguistic data consortium an edward clarence dyason fellowship and the universities of pennsylvania edinburgh and melbourne for supporting our work on this book ',\n",
       " 'we thank julie steele abby fox loranah dimant and the rest of the o reilly team for organizing comprehensive reviews of our drafts from people across the nlp and python communities for cheerfully customizing o reilly s production tools to accommodate our needs and for meticulous copyediting work ',\n",
       " 'finally we owe a huge debt of gratitude to our partners kay mimo and jee for their love patience and support over the many years that we worked on this book ',\n",
       " 'we hope that pages our children andrew alison kirsten leonie and maaike catch our enthusiasm for language and computation from these pages ']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing \n",
    "dataset =nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',' ',dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+',' ',dataset[i])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 14,\n",
       " 'authors': 1,\n",
       " 'are': 3,\n",
       " 'indebted': 1,\n",
       " 'to': 5,\n",
       " 'following': 1,\n",
       " 'people': 2,\n",
       " 'for': 8,\n",
       " 'feedback': 1,\n",
       " 'on': 5,\n",
       " 'earlier': 1,\n",
       " 'drafts': 2,\n",
       " 'of': 7,\n",
       " 'this': 4,\n",
       " 'book': 4,\n",
       " 'doug': 2,\n",
       " 'arnold': 1,\n",
       " 'michaela': 1,\n",
       " 'atterer': 1,\n",
       " 'greg': 1,\n",
       " 'aumann': 1,\n",
       " 'kenneth': 1,\n",
       " 'beesley': 1,\n",
       " 'steven': 1,\n",
       " 'bethard': 1,\n",
       " 'ondrej': 1,\n",
       " 'bojar': 1,\n",
       " 'chris': 1,\n",
       " 'cieri': 1,\n",
       " 'robin': 2,\n",
       " 'cooper': 1,\n",
       " 'grev': 1,\n",
       " 'corbett': 1,\n",
       " 'james': 1,\n",
       " 'curran': 1,\n",
       " 'dan': 1,\n",
       " 'garrette': 1,\n",
       " 'jean': 1,\n",
       " 'mark': 2,\n",
       " 'gawron': 1,\n",
       " 'hellmann': 1,\n",
       " 'nitin': 1,\n",
       " 'indurkhya': 1,\n",
       " 'liberman': 1,\n",
       " 'peter': 1,\n",
       " 'ljunglöf': 1,\n",
       " 'stefan': 1,\n",
       " 'müller': 1,\n",
       " 'munn': 1,\n",
       " 'joel': 1,\n",
       " 'nothman': 1,\n",
       " 'adam': 1,\n",
       " 'przepiorkowski': 1,\n",
       " 'brandon': 1,\n",
       " 'rhodes': 1,\n",
       " 'stuart': 1,\n",
       " 'robinson': 1,\n",
       " 'jussi': 1,\n",
       " 'salmela': 1,\n",
       " 'kyle': 1,\n",
       " 'schlansker': 1,\n",
       " 'rob': 1,\n",
       " 'speer': 1,\n",
       " 'and': 15,\n",
       " 'richard': 1,\n",
       " 'sproat': 1,\n",
       " 'we': 6,\n",
       " 'thankful': 1,\n",
       " 'many': 2,\n",
       " 'students': 1,\n",
       " 'colleagues': 1,\n",
       " 'their': 3,\n",
       " 'comments': 1,\n",
       " 'class': 1,\n",
       " 'materials': 1,\n",
       " 'that': 3,\n",
       " 'evolved': 1,\n",
       " 'into': 1,\n",
       " 'these': 2,\n",
       " 'chapters': 1,\n",
       " 'including': 1,\n",
       " 'participants': 1,\n",
       " 'at': 1,\n",
       " 'nlp': 2,\n",
       " 'linguistics': 1,\n",
       " 'summer': 1,\n",
       " 'schools': 1,\n",
       " 'in': 2,\n",
       " 'brazil': 1,\n",
       " 'india': 1,\n",
       " 'usa': 1,\n",
       " 'would': 1,\n",
       " 'not': 1,\n",
       " 'exist': 1,\n",
       " 'without': 1,\n",
       " 'members': 1,\n",
       " 'nltk': 3,\n",
       " 'dev': 1,\n",
       " 'developer': 1,\n",
       " 'community': 1,\n",
       " 'named': 1,\n",
       " 'website': 1,\n",
       " 'who': 1,\n",
       " 'have': 1,\n",
       " 'given': 1,\n",
       " 'so': 1,\n",
       " 'freely': 1,\n",
       " 'time': 1,\n",
       " 'expertise': 1,\n",
       " 'building': 1,\n",
       " 'extending': 1,\n",
       " 'grateful': 1,\n",
       " 'u': 1,\n",
       " 's': 2,\n",
       " 'national': 1,\n",
       " 'science': 1,\n",
       " 'foundation': 1,\n",
       " 'linguistic': 1,\n",
       " 'data': 1,\n",
       " 'consortium': 1,\n",
       " 'an': 1,\n",
       " 'edward': 1,\n",
       " 'clarence': 1,\n",
       " 'dyason': 1,\n",
       " 'fellowship': 1,\n",
       " 'universities': 1,\n",
       " 'pennsylvania': 1,\n",
       " 'edinburgh': 1,\n",
       " 'melbourne': 1,\n",
       " 'supporting': 1,\n",
       " 'our': 6,\n",
       " 'work': 2,\n",
       " 'thank': 1,\n",
       " 'julie': 1,\n",
       " 'steele': 1,\n",
       " 'abby': 1,\n",
       " 'fox': 1,\n",
       " 'loranah': 1,\n",
       " 'dimant': 1,\n",
       " 'rest': 1,\n",
       " 'o': 2,\n",
       " 'reilly': 2,\n",
       " 'team': 1,\n",
       " 'organizing': 1,\n",
       " 'comprehensive': 1,\n",
       " 'reviews': 1,\n",
       " 'from': 2,\n",
       " 'across': 1,\n",
       " 'python': 1,\n",
       " 'communities': 1,\n",
       " 'cheerfully': 1,\n",
       " 'customizing': 1,\n",
       " 'production': 1,\n",
       " 'tools': 1,\n",
       " 'accommodate': 1,\n",
       " 'needs': 1,\n",
       " 'meticulous': 1,\n",
       " 'copyediting': 1,\n",
       " 'finally': 1,\n",
       " 'owe': 1,\n",
       " 'a': 1,\n",
       " 'huge': 1,\n",
       " 'debt': 1,\n",
       " 'gratitude': 1,\n",
       " 'partners': 1,\n",
       " 'kay': 1,\n",
       " 'mimo': 1,\n",
       " 'jee': 1,\n",
       " 'love': 1,\n",
       " 'patience': 1,\n",
       " 'support': 1,\n",
       " 'over': 1,\n",
       " 'years': 1,\n",
       " 'worked': 1,\n",
       " 'hope': 1,\n",
       " 'pages': 2,\n",
       " 'children': 1,\n",
       " 'andrew': 1,\n",
       " 'alison': 1,\n",
       " 'kirsten': 1,\n",
       " 'leonie': 1,\n",
       " 'maaike': 1,\n",
       " 'catch': 1,\n",
       " 'enthusiasm': 1,\n",
       " 'language': 1,\n",
       " 'computation': 1}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating histogram\n",
    "wordcount = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in wordcount.keys():\n",
    "            wordcount[word] = 1\n",
    "        else:\n",
    "            wordcount[word] = wordcount[word] + 1\n",
    "wordcount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'the',\n",
       " 'for',\n",
       " 'of',\n",
       " 'we',\n",
       " 'our',\n",
       " 'to',\n",
       " 'on',\n",
       " 'this',\n",
       " 'book',\n",
       " 'are',\n",
       " 'their',\n",
       " 'that',\n",
       " 'nltk',\n",
       " 'people',\n",
       " 'drafts',\n",
       " 'doug',\n",
       " 'robin',\n",
       " 'mark',\n",
       " 'many',\n",
       " 'these',\n",
       " 'nlp',\n",
       " 'in',\n",
       " 's',\n",
       " 'work',\n",
       " 'o',\n",
       " 'reilly',\n",
       " 'from',\n",
       " 'pages',\n",
       " 'authors',\n",
       " 'indebted',\n",
       " 'following',\n",
       " 'feedback',\n",
       " 'earlier',\n",
       " 'arnold',\n",
       " 'michaela',\n",
       " 'atterer',\n",
       " 'greg',\n",
       " 'aumann',\n",
       " 'kenneth',\n",
       " 'beesley',\n",
       " 'steven',\n",
       " 'bethard',\n",
       " 'ondrej',\n",
       " 'bojar',\n",
       " 'chris',\n",
       " 'cieri',\n",
       " 'cooper',\n",
       " 'grev',\n",
       " 'corbett']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq #to find n largest value\n",
    "\n",
    "freq_words = heapq.nlargest(50,wordcount,key=wordcount.get)\n",
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    x.append(vector)\n",
    "#x\n",
    "\n",
    "data\n",
    "freq_words\n",
    "vector\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(x)\n",
    "x[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0.6931471805599453, 'the': 0.7731898882334819, 'for': 0.7731898882334819, 'of': 0.8754687373538999, 'we': 0.8754687373538999, 'our': 1.0116009116784799, 'to': 0.8754687373538999, 'on': 0.8754687373538999, 'this': 1.0116009116784799, 'book': 1.0116009116784799, 'are': 1.2039728043259361, 'their': 1.2039728043259361, 'that': 1.2039728043259361, 'nltk': 2.0794415416798357, 'people': 1.5040773967762742, 'drafts': 1.5040773967762742, 'doug': 2.0794415416798357, 'robin': 2.0794415416798357, 'mark': 2.0794415416798357, 'many': 1.5040773967762742, 'these': 1.5040773967762742, 'nlp': 1.5040773967762742, 'in': 1.5040773967762742, 's': 1.5040773967762742, 'work': 1.5040773967762742, 'o': 2.0794415416798357, 'reilly': 2.0794415416798357, 'from': 1.5040773967762742, 'pages': 2.0794415416798357, 'authors': 2.0794415416798357, 'indebted': 2.0794415416798357, 'following': 2.0794415416798357, 'feedback': 2.0794415416798357, 'earlier': 2.0794415416798357, 'arnold': 2.0794415416798357, 'michaela': 2.0794415416798357, 'atterer': 2.0794415416798357, 'greg': 2.0794415416798357, 'aumann': 2.0794415416798357, 'kenneth': 2.0794415416798357, 'beesley': 2.0794415416798357, 'steven': 2.0794415416798357, 'bethard': 2.0794415416798357, 'ondrej': 2.0794415416798357, 'bojar': 2.0794415416798357, 'chris': 2.0794415416798357, 'cieri': 2.0794415416798357, 'cooper': 2.0794415416798357, 'grev': 2.0794415416798357, 'corbett': 2.0794415416798357}\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF..\n",
    "# used to solve some disadvantages of bag of words,,,,\n",
    "# tf = (no. of Occurrence of word in a document/no. of words)\n",
    "#IDF = log(no. of documents / no. of documents containing words)\n",
    "word_idf = {}\n",
    "\n",
    "for word in freq_words:\n",
    "    doc_count = 0\n",
    "    for data in dataset:\n",
    "#         for wordd in nltk.word_tokenize(data):\n",
    "#             if word == wordd:\n",
    "                \n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count = doc_count + 1\n",
    "    word_idf[word] = np.log((len(dataset)/doc_count)+1)\n",
    "\n",
    "print(word_idf)\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': [0.014285714285714285,\n",
       "  0.08823529411764706,\n",
       "  0.058823529411764705,\n",
       "  0.058823529411764705,\n",
       "  0.0625,\n",
       "  0.06451612903225806,\n",
       "  0.09090909090909091],\n",
       " 'the': [0.02857142857142857,\n",
       "  0.058823529411764705,\n",
       "  0.08823529411764706,\n",
       "  0.08823529411764706,\n",
       "  0.0625,\n",
       "  0.03225806451612903,\n",
       "  0.0],\n",
       " 'for': [0.014285714285714285,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.029411764705882353,\n",
       "  0.0625,\n",
       "  0.03225806451612903,\n",
       "  0.045454545454545456],\n",
       " 'of': [0.014285714285714285,\n",
       "  0.0,\n",
       "  0.058823529411764705,\n",
       "  0.029411764705882353,\n",
       "  0.041666666666666664,\n",
       "  0.03225806451612903,\n",
       "  0.0],\n",
       " 'we': [0.0,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.029411764705882353,\n",
       "  0.020833333333333332,\n",
       "  0.06451612903225806,\n",
       "  0.045454545454545456],\n",
       " 'our': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.029411764705882353,\n",
       "  0.041666666666666664,\n",
       "  0.03225806451612903,\n",
       "  0.09090909090909091],\n",
       " 'to': [0.014285714285714285,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.029411764705882353,\n",
       "  0.020833333333333332,\n",
       "  0.03225806451612903,\n",
       "  0.0],\n",
       " 'on': [0.014285714285714285,\n",
       "  0.029411764705882353,\n",
       "  0.029411764705882353,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.03225806451612903,\n",
       "  0.0],\n",
       " 'this': [0.014285714285714285,\n",
       "  0.0,\n",
       "  0.029411764705882353,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.03225806451612903,\n",
       "  0.0],\n",
       " 'book': [0.014285714285714285,\n",
       "  0.0,\n",
       "  0.029411764705882353,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.03225806451612903,\n",
       "  0.0],\n",
       " 'are': [0.014285714285714285,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'their': [0.0,\n",
       "  0.029411764705882353,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.03225806451612903,\n",
       "  0.0],\n",
       " 'that': [0.0,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.03225806451612903,\n",
       "  0.045454545454545456],\n",
       " 'nltk': [0.0, 0.0, 0.08823529411764706, 0.0, 0.0, 0.0, 0.0],\n",
       " 'people': [0.014285714285714285,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.020833333333333332,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'drafts': [0.014285714285714285,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.020833333333333332,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'doug': [0.02857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'robin': [0.02857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mark': [0.02857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'many': [0.0, 0.029411764705882353, 0.0, 0.0, 0.0, 0.03225806451612903, 0.0],\n",
       " 'these': [0.0,\n",
       "  0.029411764705882353,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456],\n",
       " 'nlp': [0.0, 0.029411764705882353, 0.0, 0.0, 0.020833333333333332, 0.0, 0.0],\n",
       " 'in': [0.0, 0.029411764705882353, 0.029411764705882353, 0.0, 0.0, 0.0, 0.0],\n",
       " 's': [0.0, 0.0, 0.0, 0.029411764705882353, 0.020833333333333332, 0.0, 0.0],\n",
       " 'work': [0.0, 0.0, 0.0, 0.029411764705882353, 0.020833333333333332, 0.0, 0.0],\n",
       " 'o': [0.0, 0.0, 0.0, 0.0, 0.041666666666666664, 0.0, 0.0],\n",
       " 'reilly': [0.0, 0.0, 0.0, 0.0, 0.041666666666666664, 0.0, 0.0],\n",
       " 'from': [0.0, 0.0, 0.0, 0.0, 0.020833333333333332, 0.0, 0.045454545454545456],\n",
       " 'pages': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091],\n",
       " 'authors': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'indebted': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'following': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'feedback': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'earlier': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arnold': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'michaela': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atterer': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'greg': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aumann': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'kenneth': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'beesley': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'steven': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bethard': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ondrej': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bojar': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'chris': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cieri': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cooper': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'grev': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'corbett': [0.014285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf matrix\n",
    "\n",
    "tf_matrix = {}\n",
    "for word in freq_words:\n",
    "    doct_tf = []\n",
    "    for data in dataset:\n",
    "        freq = 0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if w == word:\n",
    "                freq = freq + 1\n",
    "        tf_word = freq/(len(nltk.word_tokenize(data)))\n",
    "        #print(tf_word)\n",
    "        doct_tf.append(tf_word)\n",
    "    tf_matrix[word] = doct_tf\n",
    "tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.00990210257942779,\n",
       "  0.06116004534352459,\n",
       "  0.04077336356234972,\n",
       "  0.04077336356234972,\n",
       "  0.04332169878499658,\n",
       "  0.04471917293935131,\n",
       "  0.06301338005090412],\n",
       " [0.022091139663813767,\n",
       "  0.045481758131381285,\n",
       "  0.06822263719707193,\n",
       "  0.06822263719707193,\n",
       "  0.04832436801459262,\n",
       "  0.024941609297854255,\n",
       "  0.0],\n",
       " [0.011045569831906884,\n",
       "  0.022740879065690642,\n",
       "  0.0,\n",
       "  0.022740879065690642,\n",
       "  0.04832436801459262,\n",
       "  0.024941609297854255,\n",
       "  0.03514499491970372],\n",
       " [0.012506696247912855,\n",
       "  0.0,\n",
       "  0.05149816102081764,\n",
       "  0.02574908051040882,\n",
       "  0.03647786405641249,\n",
       "  0.028240927011416124,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.02574908051040882,\n",
       "  0.0,\n",
       "  0.02574908051040882,\n",
       "  0.018238932028206246,\n",
       "  0.05648185402283225,\n",
       "  0.03979403351608636],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.029752967990543524,\n",
       "  0.04215003798660333,\n",
       "  0.03263228747349935,\n",
       "  0.09196371924349818],\n",
       " [0.012506696247912855,\n",
       "  0.02574908051040882,\n",
       "  0.0,\n",
       "  0.02574908051040882,\n",
       "  0.018238932028206246,\n",
       "  0.028240927011416124,\n",
       "  0.0],\n",
       " [0.012506696247912855,\n",
       "  0.02574908051040882,\n",
       "  0.02574908051040882,\n",
       "  0.02574908051040882,\n",
       "  0.0,\n",
       "  0.028240927011416124,\n",
       "  0.0],\n",
       " [0.014451441595406855,\n",
       "  0.0,\n",
       "  0.029752967990543524,\n",
       "  0.029752967990543524,\n",
       "  0.0,\n",
       "  0.03263228747349935,\n",
       "  0.0],\n",
       " [0.014451441595406855,\n",
       "  0.0,\n",
       "  0.029752967990543524,\n",
       "  0.029752967990543524,\n",
       "  0.0,\n",
       "  0.03263228747349935,\n",
       "  0.0],\n",
       " [0.017199611490370515,\n",
       "  0.03541096483311577,\n",
       "  0.0,\n",
       "  0.03541096483311577,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.03541096483311577,\n",
       "  0.03541096483311577,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.03883783239761084,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.03541096483311577,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.03883783239761084,\n",
       "  0.05472603656026982],\n",
       " [0.0, 0.0, 0.18348013603057375, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.021486819953946773, 0.0, 0.0, 0.0, 0.03133494576617238, 0.0, 0.0],\n",
       " [0.021486819953946773, 0.0, 0.0, 0.0, 0.03133494576617238, 0.0, 0.0],\n",
       " [0.05941261547656673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.05941261547656673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.05941261547656673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.04423757049341983, 0.0, 0.0, 0.0, 0.048518625702460454, 0.0],\n",
       " [0.0, 0.04423757049341983, 0.0, 0.0, 0.0, 0.0, 0.06836715439892156],\n",
       " [0.0, 0.04423757049341983, 0.0, 0.0, 0.03133494576617238, 0.0, 0.0],\n",
       " [0.0, 0.04423757049341983, 0.04423757049341983, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.04423757049341983, 0.03133494576617238, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.04423757049341983, 0.03133494576617238, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.08664339756999315, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.08664339756999315, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.03133494576617238, 0.0, 0.06836715439892156],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18904014015271234],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.029706307738283366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix = []\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = []\n",
    "    for value in tf_matrix[word]:\n",
    "        score = value * word_idf[word]\n",
    "        tfidf.append(score)\n",
    "    tfidf_matrix.append(tfidf)\n",
    "tfidf_matrix\n",
    "#vector of tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 50)\n",
      "[[0.0099021  0.02209114 0.01104557 0.0125067  0.         0.\n",
      "  0.0125067  0.0125067  0.01445144 0.01445144 0.01719961 0.\n",
      "  0.         0.         0.02148682 0.02148682 0.05941262 0.05941262\n",
      "  0.05941262 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02970631\n",
      "  0.02970631 0.02970631 0.02970631 0.02970631 0.02970631 0.02970631\n",
      "  0.02970631 0.02970631 0.02970631 0.02970631 0.02970631 0.02970631\n",
      "  0.02970631 0.02970631 0.02970631 0.02970631 0.02970631 0.02970631\n",
      "  0.02970631 0.02970631]\n",
      " [0.06116005 0.04548176 0.02274088 0.         0.02574908 0.\n",
      "  0.02574908 0.02574908 0.         0.         0.03541096 0.03541096\n",
      "  0.03541096 0.         0.         0.         0.         0.\n",
      "  0.         0.04423757 0.04423757 0.04423757 0.04423757 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.04077336 0.06822264 0.         0.05149816 0.         0.\n",
      "  0.         0.02574908 0.02975297 0.02975297 0.         0.03541096\n",
      "  0.         0.18348014 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.04423757 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.04077336 0.06822264 0.02274088 0.02574908 0.02574908 0.02975297\n",
      "  0.02574908 0.02574908 0.02975297 0.02975297 0.03541096 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04423757\n",
      "  0.04423757 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.0433217  0.04832437 0.04832437 0.03647786 0.01823893 0.04215004\n",
      "  0.01823893 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.03133495 0.03133495 0.         0.\n",
      "  0.         0.         0.         0.03133495 0.         0.03133495\n",
      "  0.03133495 0.0866434  0.0866434  0.03133495 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.04471917 0.02494161 0.02494161 0.02824093 0.05648185 0.03263229\n",
      "  0.02824093 0.02824093 0.03263229 0.03263229 0.         0.03883783\n",
      "  0.03883783 0.         0.         0.         0.         0.\n",
      "  0.         0.04851863 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.06301338 0.         0.03514499 0.         0.03979403 0.09196372\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05472604 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06836715 0.         0.         0.\n",
      "  0.         0.         0.         0.06836715 0.18904014 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.asanyarray(tfidf_matrix)\n",
    "x = np.transpose(x)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
